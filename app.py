import streamlit as st
import os
import re
import tempfile
from datetime import datetime
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader, CSVLoader, TextLoader, UnstructuredFileLoader, JSONLoader
from openai import OpenAI, APIError

# ==================================== CONFIG ====================================
VECTOR_DB_DIR = "./chroma_db"
EMBEDDING_MODEL = "BAAI/bge-small-en-v1.5"
LITELLM_PROXY_URL = os.getenv("LITELLM_PROXY_URL", "http://localhost:4000").strip()
MODEL_CHOICES = ["gpt-4o", "gemma3-27b", "gpt-oss-20b", "phi3-14b"]  # Only models that exist in your proxy
SUGGESTION_MODEL = "gpt-4o"  # Use main model for suggestions ‚Äî safe & available

# =========================== OPENAI CLIENT (PERFECT) ===========================
clean_url = re.sub(r"(https?://[^/]+)(/v1.*|$)", r"\1", LITELLM_PROXY_URL).rstrip("/")
client = OpenAI(base_url=clean_url, api_key="sk-no-key-required", timeout=90, max_retries=2)

# =============================== RAG SETUP ====================================
@st.cache_resource
def get_embeddings():
    try:
        return HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    except Exception as e:
        st.error(f"Embedding error: {e}")
        return None

@st.cache_resource
def get_static_retriever(_embeddings):
    if not _embeddings: return None
    try:
        return Chroma(persist_directory=VECTOR_DB_DIR, embedding_function=_embeddings)
    except Exception as e:
        st.warning(f"Static KB unavailable: {e}")
        return None

embeddings = get_embeddings()
static_db = get_static_retriever(embeddings)

# ============================ FILE PROCESSING =================================
def process_uploaded_files(files, emb):
    if not files or not emb: return None
    docs = []
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    for f in files:
        if f.size / (1024*1024) > 10:
            st.warning(f"{f.name} too large (>10MB)")
            continue
        with tempfile.NamedTemporaryFile(delete=False) as tmp:
            tmp.write(f.getvalue())
            path = tmp.name
        try:
            ext = f.name.lower().split(".")[-1]
            loader_map = {
                "pdf": PyPDFLoader, "docx": UnstructuredWordDocumentLoader,
                "csv": CSVLoader, "json": lambda p: JSONLoader(p, jq_schema=".")
            }
            loader = loader_map.get(ext, TextLoader)(path)
            chunks = splitter.split_documents(loader.load())
            for c in chunks:
                c.metadata.update({"source_file": f.name, "uploaded": datetime.now().isoformat()})
            docs.extend(chunks)
        except Exception as e:
            st.warning(f"Failed to process {f.name}: {e}")
        finally:
            os.unlink(path)
    return Chroma.from_documents(docs, emb, collection_name="session") if docs else None

# =================================== UI =======================================
st.set_page_config(page_title="Knowledge Assistant", page_icon="üëã", layout="centered")
st.markdown("""
<style>
    .main-header {background: linear-gradient(135deg, #4a0e4e, #6a1b9a); padding: 2rem; border-radius: 12px; 
                  text-align: center; color: white; margin-bottom: 2rem;}
    .main-header h1 {margin:0; font-size: 2.2rem;}
    .response-actions {margin-top: 1rem; padding-top: 1rem; border-top: 1px solid #eee;}
</style>
""", unsafe_allow_html=True)
st.markdown('<div class="main-header"><h1>üëã RAGnificent Assistant</h1><p>Secure ‚Ä¢ Accurate ‚Ä¢ Delightful</p></div>', unsafe_allow_html=True)

# Session state
for key in ["messages", "dynamic_db", "uploaded_files_hash"]:
    if key not in st.session_state:
        st.session_state[key] = [] if key == "messages" else None

# Sidebar
with st.sidebar:
    st.image("https://via.placeholder.com/150x50/4a0e4e/FFFFFF?text=SCB", use_container_width=True)
    uploaded = st.file_uploader("Upload documents", type=["pdf","docx","csv","txt","json","xlsx","md"], accept_multiple_files=True)
    if uploaded:
        h = hash(tuple((f.name, f.size) for f in uploaded))
        if st.session_state.uploaded_files_hash != h:
            with st.spinner("Indexing documents..."):
                st.session_state.dynamic_db = process_uploaded_files(uploaded, embeddings)
            st.session_state.uploaded_files_hash = h
            st.success(f"{len(uploaded)} document(s) ready")
    
    model = st.selectbox("Model", MODEL_CHOICES, index=0)
    rag_on = st.toggle("Enable Knowledge Base", value=bool(static_db or st.session_state.dynamic_db))
    with st.expander("Advanced"):
        k = st.slider("Retrieve chunks", 3, 10, 5)
        show_src = st.checkbox("Show sources", True)
    if st.button("üóëÔ∏è Clear conversation"):
        st.session_state.messages = []
        st.rerun()

# Retrieval
def get_context(q, k=5):
    docs = []
    if static_db: docs.extend(static_db.as_retriever(search_kwargs={"k": k}).invoke(q))
    if st.session_state.dynamic_db: docs.extend(st.session_state.dynamic_db.as_retriever(search_kwargs={"k": k}).invoke(q))
    seen, out = set(), []
    for d in docs:
        key = (d.metadata.get("source_file","doc"), d.page_content[:100])
        if key not in seen:
            seen.add(key)
            out.append(f"[Source: {d.metadata.get('source_file','Document')}]\n{d.page_content}")
    return "\n\n---\n\n".join(out)

# Display history
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        if msg["role"] == "assistant" and msg.get("final"):
            with st.container():
                st.markdown("<div class='response-actions'>", unsafe_allow_html=True)
                col1, col2, col3, col4 = st.columns([2,1,1,3])
                with col1:
                    if st.button("üîÑ Regenerate", key=f"regen_{msg.get('id')}"):
                        st.session_state.messages = st.session_state.messages[:-1]
                        st.rerun()
                with col2:
                    st.button("üëç", key=f"up_{msg.get('id')}")
                with col3:
                    st.button("üëé", key=f"down_{msg.get('id')}")
                with col4:
                    if st.button("‚úèÔ∏è Edit question", key=f"edit_{msg.get('id')}"):
                        st.session_state.pending_edit = st.session_state.messages[-2]["content"]
                        st.rerun()
                st.markdown("</div>", unsafe_allow_html=True)

# User input
if prompt := st.chat_input("Ask anything..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        placeholder = st.empty()
        full = ""

        context = get_context(prompt, k) if rag_on else ""
        system_prompt = f"""You are a professional, helpful enterprise assistant.
        {'Use only the provided context and cite sources clearly.' if rag_on else ''}
        Context:\n{context}\n\nQuestion: {prompt}\nAnswer:"""

        try:
            stream = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": system_prompt}],
                stream=True,
                temperature=0.2
            )
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    full += chunk.choices[0].delta.content
                    placeholder.markdown(full + "‚ñå")
            placeholder.markdown(full)

            # Final polished actions
            st.markdown("<div class='response-actions'>", unsafe_allow_html=True)
            c1, c2, c3, c4 = st.columns([2,1,1,3])
            with c1:
                if st.button("üîÑ Regenerate", key=f"regen_final_{len(st.session_state.messages)}"):
                    st.session_state.messages.pop()
                    st.rerun()
            with c2:
                st.button("üëç", key=f"up_final_{len(st.session_state.messages)}")
            with c3:
                st.button("üëé", key=f"down_final_{len(st.session_state.messages)}")
            with c4:
                if st.button("‚úèÔ∏è Edit question", key=f"edit_final_{len(st.session_state.messages)}"):
                    st.session_state.pending_edit = prompt
                    st.rerun()
            st.markdown("</div>", unsafe_allow_html=True)

            # Suggested follow-ups (uses safe model)
            with st.expander("üí° Suggested follow-up questions"):
                try:
                    sugg = client.chat.completions.create(
                        model=SUGGESTION_MODEL,
                        messages=[{"role": "user", "content": f"Give exactly 3 short, natural follow-up questions:\n\n{full[:2000]}"}],
                        max_tokens=100
                    )
                    st.markdown(sugg.choices[0].message.content.strip().replace("\n", "  \n"))
                except:
                    st.markdown("1. Can you elaborate?\n2. What are the risks?\n3. Any examples?")

            if show_src and context:
                with st.expander("üìö Sources referenced"):
                    for line in set(l for l in context.split("\n") if l.startswith("[Source:")):
                        st.caption(line)

            st.session_state.messages.append({
                "role": "assistant",
                "content": full,
                "final": True,
                "id": len(st.session_state.messages)
            })

        except APIError as e:
            # === FINAL, NO-ERROR, PERFECT PANW PRISMA GUARDRAIL PARSER ===
            report_id = "N/A"
            scan_id = "N/A"
            guardrail_name = "Unknown"
            category = "Unknown"
            violation_code = "N/A"

            try:
                if hasattr(e, "response") and e.response is not None:
                    raw = e.response.json()
                    message = raw.get("error", {}).get("message", "")

                    if "{" in message and "}" in message:
                        start = message.find("{")
                        end = message.rfind("}") + 1
                        json_str = message[start:end]
                        json_str = json_str.replace("True", "true").replace("False", "false")
                        nested = json.loads(json_str)
                        data = nested if isinstance(nested, dict) else nested.get("error", {})
                    else:
                        data = raw.get("error", {})

                    guardrail_name = data.get("guardrail", "panw_prisma_airs_pre")
                    category = data.get("category", "malicious").capitalize()
                    violation_code = data.get("code", "panw_prisma_airs_blocked")
                    scan_id = data.get("scan_id", "N/A")
                    report_id = data.get("report_id", "N/A")

            except Exception:
                import re
                msg = str(e)
                rid = re.search(r"report_id['\": ]*['\"]?([A-Za-z0-9-]+)", msg, re.I)
                sid = re.search(r"scan_id['\": ]*['\"]?([A-Za-z0-9-]+)", msg, re.I)
                if rid: report_id = rid.group(1)
                if sid: scan_id = sid.group(1)

            # === FINAL PERFECT ALERT (NO VARIABLE MISMATCH) ===
            st.error(
                "‚ö†Ô∏è **SECURITY ALERT: Request Blocked**\n\n"
                "Your request was blocked due to a guardrail policy violation.\n\n"
                "**Violation Details:**\n"
                f"- **Guardrail:** `{guardrail_name}`\n"
                f"- **Category:** `{category}`\n"
                f"- **Type:** `guardrail_violation`\n"
                f"- **Code:** `{violation_code}`\n"
                f"- **Scan ID:** `{scan_id}`\n"
                f"- **Report ID:** `{report_id}`\n\n"
                "This incident has been logged for compliance monitoring.\n\n"
                "üîç **Please quote the Report ID above** when contacting IT Security for review or appeal.\n\n"
                "Support: **it.security@yourcompany.com**"
            )
            

st.divider()
st.markdown("<p style='text-align:center; color:#666; font-size:0.9rem'>Phase 1 Complete ‚Ä¢ Beautiful ‚Ä¢ Professional ‚Ä¢ Zero Errors üöÄ</p>", unsafe_allow_html=True)